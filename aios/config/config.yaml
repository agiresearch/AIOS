# Global Configuration for AIOS

# API Keys Configuration
api_keys:
  openai: ""    # OpenAI API key
  gemini: ""    # Google Gemini API key
  groq: ""      # Groq API key
  anthropic: "" # Anthropic API key
  huggingface:
    auth_token: ""  # Your HuggingFace auth token for authorized models
    cache_dir: ""   # Your cache directory for saving huggingface models

# LLM Configuration
llms:
  models:

    # OpenAI Models
    # - name: "gpt-4.1-mini"
    #   backend: "openai"

    # - name: "gpt-4.1-nano"
    #   backend: "openai"

    # - name: "gpt-4.1"
    #   backend: "openai"

    # - name: "gpt-4o"
    #   backend: "openai"

    # - name: "gpt-4o-mini"
    #   backend: "openai"


    # Google Models
    # - name: "gemini-2.0-flash"
    #   backend: "gemini"


    # Anthropic Models
    # - name: "claude-3-opus"
    #   backend: "anthropic"

    # Ollama Models
    - name: "qwen3:1.7b"
      backend: "ollama"
      hostname: "http://localhost:11434" # Make sure to run ollama server
    
    - name: "qwen3:4b"
      backend: "ollama"
      hostname: "http://localhost:11434" # Make sure to run ollama server

    # HuggingFace Models
    # - name: "meta-llama/Llama-3.1-8B-Instruct"
    #   backend: "huggingface"
    #   max_gpu_memory: {0: "48GB"}  # GPU memory allocation
    #   eval_device: "cuda:0"  # Device for model evaluation

    # vLLM Models
    # To use vllm as backend, you need to install vllm and run the vllm server https://docs.vllm.ai/en/latest/serving/openai_compatible_server.html
    # An example command to run the vllm server is:
    # vllm serve meta-llama/Llama-3.2-3B-Instruct --port 8091
    # - name: "meta-llama/Llama-3.1-8B-Instruct"
    #  backend: "vllm"
    #  hostname: "http://localhost:8091"

  router:
    strategy: "sequential"
    bootstrap_url: "https://drive.google.com/file/d/1SF7MAvtnsED7KMeMdW3JDIWYNGPwIwL7/view"

  log_mode: "console" # choose from [console, file]
  use_context_manager: false

memory:
  log_mode: "console" # choose from [console, file]

storage:
  root_dir: "root"
  use_vector_db: true
  # vector DB backend: chroma | qdrant
  vector_db_backend: "chroma"
  # Qdrant connection (used when backend == qdrant). Can also use env vars QDRANT_HOST, QDRANT_PORT, QDRANT_API_KEY
  qdrant_host: "localhost"
  qdrant_port: 6333
  qdrant_api_key: ""
  # Embedding model for Qdrant fastembed integration
  qdrant_model_name: "sentence-transformers/all-MiniLM-L6-v2"

tool:
  mcp_server_script_path: "aios/tool/mcp_server.py"

scheduler:
  log_mode: "console" # choose from [console, file]

agent_factory:
  log_mode: "console" # choose from [console, file]
  max_workers: 64

server:
  host: "localhost"
  port: 8000
