from aios.context.simple_context import SimpleContextManager
from aios.llm_core.strategy import RouterStrategy, SimpleStrategy
from aios.llm_core.local import HfLocalBackend, VLLMLocalBackend
from aios.utils.id_generator import generator_tool_call_id
from cerebrum.llm.communication import Response
from litellm import completion
import json

from typing import Dict, Optional
import time
import re
import os

class LLMAdapter:
    """
    The LLMAdapter class is an abstraction layer that represents the LLM
    router. This router allows load-balancing of multiple varying endpoints so
    that multiple requests can be handled at once.

    Args:
        llm_name (str or List[str])     : Name of the LLMs. If a string is
                                          provided, then only one LLM will be
                                          activated.
        max_gpu_memory (dict, optional) : Maximum GPU resources that can be 
                                          allocated to the LLMs.
        eval_device (str, optional)     : Evaluation device of binding LLM to
                                          designated devices for inference.
        max_new_tokens (int, optional)  : Maximum token length generated by the
                                          LLM. Defaults to 256.
        log_mode (str, optional)        : Mode of logging the LLM processing
                                          status. Defaults to "console".
        llm_backend (str, optional)     : Backend to use for speeding up 
                                          open-source LLMs. Defaults to None.
                                          Choices are ["vllm", "ollama"]
    """

    def __init__(
        self,
        llm_name: str | list[str],
        max_gpu_memory: Optional[Dict] = None,
        eval_device: Optional[str] = None,
        max_new_tokens: int = 256,
        log_mode: str = "console",
        llm_backend: Optional[str | list[str]] = None,
        use_context_manager: bool = False,
        strategy: Optional[RouterStrategy] = RouterStrategy.SIMPLE,
        api_key: str | list[str] | None = None,
    ):
        """Initialize the LLM with the specified configuration.
        
        Args:
            llm_name            : Name of the LLM model to use
            max_gpu_memory      : Maximum GPU memory allocation per device
            eval_device         : Device to use for evaluation
            max_new_tokens      : Maximum number of new tokens to generate
            log_mode            : Logging mode ("console" or other options)
            use_backend         : Specific backend to use (if None, inferred
                                  from model name)
            use_context_manager : Whether to use context manager
            api_key             : DEPRECATED. This was originally used to store
                                  an API Key for the LLM, but LiteLLM uses keys
                                  directly from the process environment
                                  variables making this needless.
        """
        if isinstance(llm_name, list) != isinstance(llm_backend, list):
            raise ValueError
        elif isinstance(llm_backend, list) and len(llm_name) == len(llm_backend):
            raise ValueError

        self.llm_name            = llm_name if isinstance(llm_name, list) else [llm_name]
        self.max_gpu_memory      = max_gpu_memory
        self.eval_device         = eval_device
        self.max_new_tokens      = max_new_tokens
        self.log_mode            = log_mode
        self.llm_backend         = llm_backend if isinstance(llm_backend, list) else [llm_backend]
        self.context_manager     = SimpleContextManager() if use_context_manager else None

        match strategy:
            case RouterStrategy.SIMPLE:
                self.strategy = SimpleStrategy(self.llm_name)

        # Backwards compatibility for pre-router LLM names.
        if os.environ["HF_AUTH_TOKENS"]:
            os.environ["HUGGING_FACE_API_KEY"] = os.environ["HF_AUTH_TOKENS"]

        for idx in range(len(self.llm_name)):
            if self.llm_backend[idx] is None:
                continue

            match self.llm_backend[idx]:
                case "hflocal":
                    raise NotImplemented
                case "vllm":
                    raise NotImplemented
                case None:
                    continue
                case _:
                    prefix = self.llm_backend[idx] + "/"
                    is_formatted = self.llm_name[idx].startswith(prefix)
                    if not is_formatted:
                        self.llm_name[idx] = prefix + self.llm_name[idx]

    def tool_calling_input_format(self, messages: list, tools: list) -> list:
        """Integrate tool information into the messages for open-sourced LLMs

        Args:
            messages (list): messages with different roles
            tools (list): tool information
        """
        prefix_prompt = (
            "In and only in current step, you need to call tools. Available tools are: "
        )
        tool_prompt = json.dumps(tools)
        suffix_prompt = "".join(
            [
                "Must call functions that are available. To call a function, respond "
                "immediately and only with a list of JSON object of the following format:"
                '{[{"name":"function_name_value","parameters":{"parameter_name1":"parameter_value1",'
                '"parameter_name2":"parameter_value2"}}]}'
            ]
        )

        # translate tool call message for models don't support tool call
        for message in messages:
            if "tool_calls" in message:
                message["content"] = json.dumps(message.pop("tool_calls"))
            elif message["role"] == "tool":
                message["role"] = "user"
                tool_call_id = message.pop("tool_call_id")
                content = message.pop("content")
                message["content"] = (
                    f"The result of the execution of function(id :{tool_call_id}) is: {content}. "
                )

        messages[-1]["content"] += prefix_prompt + tool_prompt + suffix_prompt
        return messages

    def parse_json_format(self, message: str) -> str:
        json_array_pattern = r"\[\s*\{.*?\}\s*\]"
        json_object_pattern = r"\{\s*.*?\s*\}"

        match_array = re.search(json_array_pattern, message)

        if match_array:
            json_array_substring = match_array.group(0)

            try:
                json_array_data = json.loads(json_array_substring)
                return json.dumps(json_array_data)
            except json.JSONDecodeError:
                pass

        match_object = re.search(json_object_pattern, message)

        if match_object:
            json_object_substring = match_object.group(0)

            try:
                json_object_data = json.loads(json_object_substring)
                return json.dumps(json_object_data)
            except json.JSONDecodeError:
                pass
        return "[]"

    def parse_tool_calls(self, message):
        # add tool call id and type for models don't support tool call
        tool_calls = json.loads(self.parse_json_format(message))
        for tool_call in tool_calls:
            tool_call["id"] = generator_tool_call_id()
            tool_call["type"] = "function"
        return tool_calls

    def address_syscall(
        self,
        llm_syscall,
        temperature=0.0
    ):
        """
        Address request sent from the agent

        Args:
            llm_syscall (LLMSyscall)      : LLMSyscall object that contains
                                            request sent from the agent
            temperature (float, optional) : Parameter to control the randomness
                                            of LLM output. Defaults to 0.0.
        """
        messages = llm_syscall.query.messages
        tools    = llm_syscall.query.tools
        ret_type = llm_syscall.query.message_return_type

        llm_syscall.set_status("executing")
        llm_syscall.set_start_time(time.time())

        restored_context = None
        if self.context_manager:
            pid = llm_syscall.get_pid()
            if self.context_manager.check_restoration(pid):
                restored_context = self.context_manager.gen_recover(pid)

        if restored_context is not None:
            messages += [{
                "role": "assistant",
                "content": "" + restored_context,
            }]

        if tools:
            messages = self.tool_calling_input_format(messages, tools)

        model = self.strategy()

        if isinstance(model, str):
            res = str(completion(
                model=model,
                messages=messages,
                temperature=temperature,
            ))
        elif isinstance(model, HfLocalBackend):
            res = model(
                messages=messages,
                temperature=temperature,
            )
        elif isinstance(model, VLLMLocalBackend):
            res = model(
                messages=messages,
                temperature=temperature,
            )
        else:
            # Any other type of llm_name should error.
            raise RuntimeError

        if tools:
            if tool_calls := self.parse_tool_calls(res):
                return Response(response_message=None,
                                tool_calls=tool_calls,
                                finished=True)

        if ret_type == "json":
            res = self.parse_json_format(res)

        return Response(response_message=res, finished=True)
